DSC345 Final Project Script for Jaime:
BACKGROUND
For the Background, there 3 main points. Data-driven coaching, Historical Game Data, and Performance Predictors. Basketball coaches are constantly looking for data-driven strategies to boost team performance. Historically, decisions about player rotations, game strategy, and workload management have relied on intuition or basic statistics. Our project leverages extensive historical game-level data to predict NBA player points, focusing on rolling averages and standard deviations, opponent strength, and rest days on how it influences performance.
For the Significance, by integrating advanced predictive models, we provide a systematic way to evaluate the key drivers of player performance. We compare multiple methods (linear regression, random forest, and even deep learning approaches) to uncover both short-term and long-term performance trends. The project empowers coaches with quantifiable insights, enhancing traditional decision-making processes with statistical accuracy.
For the Importance, the ability to forecast player points can help teams optimize game strategies, player rotations, and manage workload effectively. By identifying patterns in a playerâ€™s performance over time, our model can contribute to better long-term development and training programs. Ultimately, the project demonstrates how predictive analytics can be applied in high-stakes environments like the NBA, potentially influencing both tactical game-day decisions and broader team management strategies.


LINEAR REGRESSION
For our next model, we chose to predict points using Linear Regression. We chose linear regression because it's one of the most popular methods used to predict the value of a numeric value based on other variables. I chose to explore using a full fit model, (basically all of the variables), forward selection, backward selection, and stepwise selection. 

For the full fit, the R-Squared is 44.83% and the Adjusted R-Squared is 14.55%. This means that all of the variables selected explain 44.83% of the data. However, the Adjusted R-Squared took a big hit and decreases significantly. The Adjusted R-Squared is penalizing the model due to how many insignificant variables we have. The F-Stat is at 1.481 with a P-Value of 0.002523 meaning we cannot rejust the null hypothosis. For coefficients and their significance, only 3 variables barely have a P-Value of less than 0.05. There are some that are slighly significant, but then the rest are not significant at all. Overall, the full model is very messy and does not have much significance in it.

For Forward Selection, the R-Squared is 32.42% and Adjusted R-Squared is 28.41%. Meaning that the variables selected explained 32.42% of the variance in the data. The R-Squared did drop 12%, but the Adjusted R-Squared is doing much better now! This also means that there are much more variables now that are actually significant. 6 variables are very significant, 5 that are significant, 8 that slighlty less significant, 1 that's barely significant, and 4 that are not significant. The F-Stat is at 8.095 so it did increase, but the P-Value decreased a lot and fell to < 2.2e-16! Which means we can reject the null hypothosis now. From the predicted vs. actual plot, we can see the line pass through the data fairly well. It's not the best but it is decent.

For Backward Selection, the R-Squared is 38.54% and the Adjusted R-Squared is 30.43%. This model is doinf better than the Forward Selection model at explaining the variance of the data! The Forward Selection had 24 predictors. The Backward Selection has 48 variables. So double what the Forward Selection has! This means that the Backward Selection is much more complicated to understand but it does perform better than the Forward Model. But, only 2 predictors are unsignificant while the rest are significant. Additionally, its F-Stat is 4.754 and P-Value of < 2.2e-16 so it can rejust the null hypothosis. From the predicted vs. actual plot, we can see the line pass through the data fairly well. It's not the best but it is decent. It does look slighlty better than the Forward Selection with the slope of it being less steep.

For Stepwise Selection, the R-Squared is 32.28% and Adjusted R-Squared is 28.44%. The P-Stat is 8.414 with a P-Value of < 2.2e-16 so we can reject the null hypothosis. Only 2 variables are insignificant in this model with the rest being significant. It's doing almost the same as Forward Selection with minor changes in the stats. But it is doing a bit worse than Backward Selection. 

When moving on to Testing, the Full Fit Model has an RMSE of 5.87 and an MAE of 4.54. This will actually be the have the lowest errors out of all of the models! But when running it, it came with a warning indicating potential multicollinearity issues. 
For the Stepwise model, the RMSE was 6.89 and MAE was 5.34. It has a higher error compared to the full model, but with a simpler set of predictors.
For the Forward Model, it had an RMSE of 6.92 and MAE of 5.40. It had very similar performacnce to the stepwise model.
For the Backward Model, it had an RMSE of 6.36 and MAE of 5.03. It performs better than the stepwise and forward models but it's still not as low as the full model.

Overall, the Backward Selection performed better than all of the other models. It had a better R-Squared, Adjusted R-Squared, and F-Statisitic with P-Value. Although it had a higher error compared to the full model, it's a good compromise because it reduces complexity compared to the full model and is less prone to overfitting and multicollinearity.